{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tI9w9UVbkmQl"
      },
      "source": [
        "## Adversarial Debiasing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oqaap7LvkmQp"
      },
      "source": [
        "### Table of contents :\n",
        "1. [Introduction](#1.-Introduction)\n",
        "2. [Data preparation](#2.-Data-preparation)\n",
        "3. [Classifier network](#3.-Classifier-network)\n",
        "\t*  [Model Fairness for Classifier Network](#Model-Fairness-for-Classifier)\n",
        "4. [Mitigate the model bias with Adversarial networks](#4.-Mitigate-the-model-bias-with-Adversarial-networks)\n",
        "\t* [Model Fairness for Adversarial networks](#Model-Fairness-for-Adversarial-networks)\n",
        "5. [Summary](#5.-Summary)\n",
        "6. [References](#References)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q-BuFSm4kmQr"
      },
      "source": [
        "## 1. Introduction "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S_7mj3YZkmQs"
      },
      "source": [
        "Welcome !\n",
        "\n",
        "We hope you have had a chance to go through the [previous sony fairness tutorials](https://github.com/sony/nnabla-examples/blob/master/interactive-demos/gender_bias_mitigation_german_cc.ipynb). In that tutorial, we had touched upon a pre-processing technique for bias mitigation ([Rewighing algroithm](https://link.springer.com/content/pdf/10.1007%2Fs10115-011-0463-8.pdf)) and also showed how to compute model fairness using different fairness criteria. \n",
        "\n",
        "In this tutorial, we give an overview of an in-processing technique for bias mitigation in predictive models. In-processing techniques involve modification of learning algorithms to address discrimination during the model training phase. [Classification with Fairness Constraints](https://arxiv.org/abs/1806.06055), [Prejudice Remover Regularizer](https://link.springer.com/content/pdf/10.1007%2F978-3-642-33486-3_3.pdf) and [Adversarial Debiasing](https://arxiv.org/pdf/1801.07593.pdf) are among the different in-processing bias mitigation techniques currently proposed from academic literature. In this tutorial we will see how to mitigate the bias in predictive model using `Adversarial Debiasing` technique.\n",
        "\n",
        "Before we go into detailed explanation, here is a sneak peek into the steps involved in the process:\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t1xEkowSkmQt"
      },
      "source": [
        "### Preparation\n",
        "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime → change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparation\n",
        "# May show warnings for newly imported packages if run in Colab default python environment.\n",
        "# Please click the `RESTART RUNTIME` to run the following script correctly.\n",
        "# The error message of conflicts is acceptable.\n",
        "!pip install nnabla-ext-cuda116\n",
        "# for cpu set context as \"!pip install nnabla\"\n",
        "!git clone https://github.com/sony/nnabla-examples.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd nnabla-examples/responsible_ai/adversarial_debiasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwp5YCehkmQu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "img = cv2.imread('images/adversarial_debiasing_workflow.png')\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9TwhQPV0kmQx"
      },
      "source": [
        "As illustrated in the picture, we will train a model to make income level predictions, analyse model fairness and then show how adversarial training can be employed to make the model fair if fairness metric is not upto the mark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npAy3Z_LkmQy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "import nnabla as nn\n",
        "from nnabla.logger import logger\n",
        "import nnabla.functions as F\n",
        "import nnabla.parametric_functions as PF\n",
        "import nnabla.solvers as S\n",
        "from nnabla.utils.data_iterator import data_iterator\n",
        "from nnabla.utils.data_source import DataSource\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from utils import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvPvEWPkmQy"
      },
      "source": [
        "## 2. Data preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-3MxNVw6kmQz"
      },
      "source": [
        "Before training a model, we need to parse [adult](https://archive.ics.uci.edu/ml/datasets/Adult) dataset into three categories i.e: features, targets, and sensitive attributes. The set of features contains the input attributes, targets contain the binary class labels (income range). Finally, sensitive attributes contain the attributes for which we want the prediction to be fair(race, sex, etc.).\n",
        "\n",
        "Note: sensitive attributes race and sex are not part of the features."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S--OlvbokmQz"
      },
      "source": [
        "Next, we split the data into train and test sets, and scale the features using standard scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wciLQUTKkmQ0"
      },
      "outputs": [],
      "source": [
        "X, y, Z = load_adult_data()\n",
        "Z = Z.drop(['sex'], axis = 1)\n",
        "\n",
        "# split into train/test set\n",
        "(X_train, X_test, y_train, y_test,\n",
        " Z_train, Z_test) = train_test_split(X, y, Z, test_size=0.5,stratify=y, random_state=7)\n",
        "\n",
        "scale_orig = StandardScaler()\n",
        "X_train = scale_orig.fit_transform(X_train)\n",
        "X_test = scale_orig.fit_transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a78FOTxKkmQ0"
      },
      "source": [
        "NNabla provides various utilities for using data for training. Here, we will use NNabla DataSource and data_iterator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etIGoT2nkmQ1"
      },
      "outputs": [],
      "source": [
        "class dataset(DataSource):\n",
        "    def __init__(self,features, labels, senstive_attribute,shuffle=False):\n",
        "        super(dataset, self).__init__(shuffle=shuffle)\n",
        "        self.x = features\n",
        "        self.y = labels.reshape(-1,1)\n",
        "        self.z = senstive_attribute\n",
        "        self._size = self.y.size\n",
        "        self._variables = ('x', 'y','z')\n",
        "        \n",
        "    def _get_data(self,idx):\n",
        "        return self.x[idx],self.y[idx],self.z[idx]\n",
        "trainset = dataset(X_train,y_train.to_numpy(),Z_train.to_numpy(),shuffle=True)\n",
        "testset = dataset(X_test,y_test.to_numpy(),Z_test.to_numpy())\n",
        "\n",
        "batachsize = 32\n",
        "trainloader = data_iterator(trainset,batch_size=batachsize)\n",
        "test_batchsize = X_test.shape[0]\n",
        "testloader = data_iterator(testset,batch_size=test_batchsize)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cZhphUUdkmQ1"
      },
      "source": [
        "## 3. Classifier network"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pGU1HTfkkmQ1"
      },
      "source": [
        "Let’s start with importing basic modules to switch between CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeJJ6CcwkmQ1"
      },
      "outputs": [],
      "source": [
        "from nnabla.ext_utils import get_extension_context\n",
        "context = \"cudnn\" # for cpu set context as 'cpu'\n",
        "device_id = 0\n",
        "ctx = get_extension_context(context, device_id=device_id)\n",
        "nn.set_default_context(ctx)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XBnuua6IkmQ2"
      },
      "source": [
        "Now we are going to create input variables for the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE8YmQ5ukmQ2"
      },
      "outputs": [],
      "source": [
        "n_feature = nn.Variable((batachsize,88))\n",
        "n_label = nn.Variable((batachsize,1))\n",
        "n_senstive = nn.Variable((batachsize,1))\n",
        "\n",
        "test_feature = nn.Variable((test_batchsize,88))\n",
        "test_label = nn.Variable((test_batchsize,1))\n",
        "test_senstive = nn.Variable((test_batchsize,1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "05d4VNNDkmQ2"
      },
      "source": [
        "Let us create a simple computation graph in NNabla to train our basic income level predictor. Our network consists of three sequential hidden layers with ReLu activation and dropout for regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7qDeweikmQ2"
      },
      "outputs": [],
      "source": [
        "def Classifier(features_n, n_hidden=32, p_dropout=0.2,train=True):\n",
        "    with nn.parameter_scope('classifier'):\n",
        "        l1 = PF.affine(features_n, n_hidden, name='l1')\n",
        "        l1 = F.relu(l1)\n",
        "        if (train):\n",
        "            l1 = F.dropout(l1,p_dropout)\n",
        "        l2 = PF.affine(l1, n_hidden, name='l2')\n",
        "        l2 = F.relu(l2)\n",
        "        if (train):\n",
        "            l2 = F.dropout(l2,p_dropout)\n",
        "        l3 = PF.affine(l2, n_hidden, name='l3')\n",
        "        l3 = F.relu(l3)\n",
        "        if (train):\n",
        "            l3 = F.dropout(l3,p_dropout)\n",
        "        l4 = PF.affine(l3, 1, name='l4')\n",
        "    return l4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cmeS_vxNkmQ3"
      },
      "source": [
        "The parameters are created and initialized randomly at function call and registered by a name `classifier` using parameter_scope context.\n",
        "\n",
        "We have defined the classifier network and it returns the output of the last affine layer.\n",
        "\n",
        "Let us create training graph first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1avd4qkkmQ3"
      },
      "outputs": [],
      "source": [
        "clf = Classifier(n_feature)\n",
        "clf.persistent = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4CYQku8nkmQ3"
      },
      "source": [
        "Creating the validation graph is almost the same. You simply need to change `train` flag to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yie4pjpLkmQ3"
      },
      "outputs": [],
      "source": [
        "v_clf = Classifier(test_feature,train=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb7SLWMzkmQ3"
      },
      "source": [
        "Let's choose sigmoid cross-entropy as the loss function and Adam optimizer as our solver."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiJQ6JNckmQ3"
      },
      "outputs": [],
      "source": [
        "loss = F.mean(F.sigmoid_cross_entropy(clf, n_label))\n",
        "\n",
        "loss.persistent = True\n",
        "\n",
        "learning_rate = 1e-03\n",
        "\n",
        "clf_solver = S.Adam(learning_rate)\n",
        "\n",
        "with nn.parameter_scope(\"classifier\"):\n",
        "    clf_solver.set_parameters(nn.get_parameters())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JPbxzpcnkmQ4"
      },
      "source": [
        "We shall train the classifier now. For each epoch, we'll iterate over the batches returned by our `data_iterator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3_dXTS-kmQ4"
      },
      "outputs": [],
      "source": [
        "max_iter = 449\n",
        "for epoch in range(10):\n",
        "    for i in range(max_iter):\n",
        "        n_feature.d, n_label.d, _ = trainloader.next()\n",
        "        clf_solver.zero_grad()\n",
        "        loss.forward(clear_no_need_grad=True)\n",
        "        loss.backward(clear_buffer=True)\n",
        "        clf_solver.update()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb2pEhUCkmQ4"
      },
      "source": [
        "The above for loop does the following for each batch:\n",
        "1. Retrieve new batch of data.\n",
        "2. Initialize gradients to zero.\n",
        "3. Execute the computation of the graph by calling the loss.forward(). Compute the loss.\n",
        "4. Execute backprop by calling loss.backward(), to compute gradients.\n",
        "5. Update the classfier weights (clf_solver.update())."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wmq2-4NkmQ4"
      },
      "source": [
        "### Model Fairness for Classifier\n",
        "Let's start our investigation classifier model fairness by analysing the predictions it made on the test set. In the previous tutorial (three metric tutorial link goes here), we've shown how to measure model fairness using different fairness criteria (Demographic parity, Equal opportunity & Equalized odds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6XghfNUkmQ4"
      },
      "outputs": [],
      "source": [
        "test_feature.d, test_label.d,test_senstive.d = testloader.next()\n",
        "v_clf_out = F.sigmoid(v_clf)\n",
        "v_clf_out.forward(clear_buffer=True)\n",
        "y_pre_clf = v_clf_out.d.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZb_j65skmQ4"
      },
      "outputs": [],
      "source": [
        "# model validation & fairness\n",
        "preds = np.where(y_pre_clf > 0.5, 1, 0)\n",
        "DPD,EOD,AAOD = get_fairness(y_test, Z_test['race'] == 1, preds)\n",
        "clf_accuracy = metrics.accuracy_score(y_test, preds) * 100"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_rYNyvzZkmQ5"
      },
      "source": [
        "In this tutorial, we adopt/compute one more well-known fairness metric - `p% rule` (Zafar et al. show in their paper [\"Fairness Constraints: Mechanisms for Fair Classification\"](https://arxiv.org/pdf/1507.05259.pdf)).\n",
        "\n",
        "The rule states that the ratio between the probability of a positive outcome given the sensitive attribute being true and the same probability given the sensitive attribute being false is no less than p:100. So, when a classifier is completely fair it will satisfy a 100%-rule. In contrast, when it is completely unfair it satisfies a %0-rule.\n",
        "\n",
        "A classifier that makes a binary class prediction $\\hat{y} \\in \\left\\{0,1 \\right\\} $ given a binary sensitive attribute $ z\\in \\left\\{0,1 \\right\\}$, one may write `p%-rule` as :\n",
        "\n",
        "$$\\min\\left(\\frac{P(\\hat{y}=1|z=1)}{P(\\hat{y}=1|z=0)}, \\frac{P(\\hat{y}=1|z=0)}{P(\\hat{y}=1|z=1)}\\right)\\geq\\frac{p}{100}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ranqu7XkkmQ5"
      },
      "outputs": [],
      "source": [
        "race_p_rule = get_p_rule(y_pre_clf, Z_test['race'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpB1kLZDkmQ5"
      },
      "source": [
        "Let's plot model fairness from above data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CcP_bNfkmQ5"
      },
      "outputs": [],
      "source": [
        "plot_fairness_multi(DPD,EOD,AAOD,race_p_rule,clf_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jHBrzZoqkmQ5"
      },
      "source": [
        "As seen in above plots, predictions are definitely not fair when considered in the context of race as a sensitive attribute.\n",
        "\n",
        "How to mitigate this bias?\n",
        "#### Mitigate the bias\n",
        "In General, we can follow two approaches\n",
        "1. Remove the bias from the dataset (pre-processing bias mitigation technique) and train the model, as illustrated in the gender bias mitigation tutorial.\n",
        "2. Constrain the model so that it is forced into making fairer predictions.\n",
        "\n",
        "In the next section, we will show you how the second approach helps in making fair decisions using adversarial network."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xrOmQCKTkmQ5"
      },
      "source": [
        "## 4. Mitigate the model bias with Adversarial networks\n",
        "\n",
        "### Adversarial Debiasing :"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L04rjUZ_kmQ6"
      },
      "source": [
        "[Adversarial debiasing](https://arxiv.org/pdf/1801.07593.pdf) procedure takes inspiration from [GANs](https://arxiv.org/abs/1406.2661) (Goodfellow et al. 2014) for training a fair classifier. In GANs they introduce a system of two neural networks in which the two neural networks compete with each other to become more accurate in their predictions. Likewise, in adversarial debiasing, we build two models:\n",
        "1. First model is a classifier which predicts target variable based on input features (training data).\n",
        "2. Second model is an adversary and it tries to predict, the sensitive attribute based on the predictions of the classifier model.\n",
        "\n",
        "Before diving into the detailed explanation of the adversary training procedure, let's define the computation graph of the adversary network."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gDpyO_UwkmQ6"
      },
      "source": [
        "Similar to the Classifier, our adversary consists of three layers. However, the input comes from a single class (the predicted income class) and the output consists of one sensitive class(race)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "423m1K11kmQ6"
      },
      "outputs": [],
      "source": [
        "def Adversary(clf_out,n_hidden=32):\n",
        "    with nn.parameter_scope(\"adversary\"):\n",
        "        Al1 = PF.affine(clf_out, n_hidden, name='Al1')\n",
        "        Al1 = F.relu(Al1)\n",
        "        Al2 = PF.affine(Al1, n_hidden, name='Al2')\n",
        "        Al2 = F.relu(Al2)\n",
        "        Al3 = PF.affine(Al2, n_hidden, name='Al3')\n",
        "        Al3 = F.relu(Al3)\n",
        "        Al4 = PF.affine(Al3,1, name='Al4')\n",
        "        return Al4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jf_x2MkSkmQ6"
      },
      "source": [
        "Let's intiailize the adversary network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RBWom6zkmQ6"
      },
      "outputs": [],
      "source": [
        "adv = Adversary(clf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mf1flphDkmQ7"
      },
      "source": [
        "For our final solution, there will be a trade-off between classifier performance and fairness for our sensitive attributes. \n",
        "\n",
        "We will tweak the adversarial loss to incorporate that trade-off: the lambda parameter weighs the adversarial loss of each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-TtHqJykmQ7"
      },
      "outputs": [],
      "source": [
        "lamdas = 130\n",
        "adv_loss = F.mean(F.mul_scalar(F.sigmoid_cross_entropy(adv, n_senstive), lamdas))\n",
        "adv_solver = S.Adam(learning_rate)\n",
        "with nn.parameter_scope(\"adversary\"):\n",
        "     adv_solver.set_parameters(nn.get_parameters())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jM_4Fq2fkmQ7"
      },
      "source": [
        "### Adversarial debias training  :"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mK2pQSzKkmQ7"
      },
      "source": [
        "As discussed in the above section, first model in adversarial training is Classifier network and the second model is an extension of the original network with the adversarial component. The figure below illustrates adversarial training procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlAj6x48kmQ7"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('images/adversarial_debiasing_training.png')\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OyKgszlCkmQ8"
      },
      "source": [
        "As shown in the above Fig, at first the Classifier model will generate predictions $\\hat{y}$ base on the input ${X}$. The Adversararial model will then try to predict the sensitive attribute values $\\hat{Z}$ from the predicted $\\hat{y}$ of the classifier. Finally, the objectives that both the models try to optimize are based on the prediction losses of the target and sensitive attributes. These are denoted by $Loss_{clf}$ and $Loss_{adv}$ in the figure.\n",
        "\n",
        "The Classifier and Adversarial networks are trained turn by turn (in the form of a zero-sum game, where one agent's gain is another agent's loss) with the objective of minimizing the loss of both the networks.\n",
        "\n",
        "For the classifier, the objective is two-fold: make the best possible predictions while ensuring that sensitive attributes cannot be derived from them. This is captured by the following objective function\n",
        "\n",
        "$$ \\min[Loss_{clf}-\\lambda Loss_{adv}]$$\n",
        "\n",
        "So, it learns to minimize its own prediction losses while maximizing that of the adversarial (due to $\\lambda$ being positive and minimizing a negated loss is the same as maximizing it). Note that increasing the size of $\\lambda$ steers the classifier network towards fairer predictions while sacrificing prediction accuracy. \n",
        "\n",
        "\n",
        "The adversary learns on the full data set and the classifier is given only the single batch, giving the adversary a slight edge in learning. The loss function for the classifier is changed to its original loss plus the weighted negative adversarial loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvOIX8n1kmQ8"
      },
      "outputs": [],
      "source": [
        "clfloss = loss - adv_loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4tFk1bbBkmQ8"
      },
      "source": [
        "In short:\n",
        "* Train the classifier on full data set for few epochs.\n",
        "\n",
        "Train the adversarial and classifier networks simultaneously: \n",
        "* Train the adversarial for a single epoch while keeping the classifier fixed.\n",
        "* Train the classifier on a single sampled mini batch while keeping the adversarial fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfDHPQoGkmQ8"
      },
      "outputs": [],
      "source": [
        "max_iter = 449\n",
        "for epoch in range(200):\n",
        "    clf.need_grad = False\n",
        "    for i in range(max_iter):\n",
        "        n_feature.d, n_label.d, n_senstive.d = trainloader.next()\n",
        "        adv_solver.zero_grad()\n",
        "        adv_loss.forward()\n",
        "        adv_loss.backward(clear_buffer=True)\n",
        "        adv_solver.update()\n",
        "        \n",
        "    for i in range(max_iter):\n",
        "        n_feature.d, n_label.d, n_senstive.d = trainloader.next()\n",
        "        pass\n",
        "    clf.need_grad = True\n",
        "    loss.forward(clear_no_need_grad=True)\n",
        "    clf_solver.zero_grad()\n",
        "    clfloss.forward(clear_no_need_grad=True)\n",
        "    clfloss.backward(clear_buffer=True)   \n",
        "    clf_solver.update()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eabX9hhlkmQ8"
      },
      "source": [
        "### Model Fairness: Adversarial network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upIC0VqukmQ8"
      },
      "outputs": [],
      "source": [
        "test_feature.d, test_label.d,test_senstive.d = testloader.next()\n",
        "v_clf_out.forward(clear_buffer=True)\n",
        "y_pre_clf = v_clf_out.d.ravel()\n",
        "preds = np.where(y_pre_clf > 0.5, 1, 0)\n",
        "DPD,EOD,AAOD = get_fairness(y_test, Z_test['race'] == 1, preds)\n",
        "clf_accuracy = metrics.accuracy_score(y_test, preds) * 100\n",
        "race_p_rule = get_p_rule(y_pre_clf, Z_test['race'])\n",
        "plot_fairness_multi(DPD,EOD,AAOD,race_p_rule,clf_accuracy,\"AfterADV\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "70-48hh-kmQ8"
      },
      "source": [
        "The plots above show how the model fairness improved (Demographic parity, Equal opportunity & Equalized odds are almost zero and the p% rule gives us the value of ~77, almost twice as likely to be fair than that of the previous approach) after adversarial debiasing training.\n",
        "\n",
        "However, the accuracy of classification dropped around ~5% (reduction in accuracy can be controlled). We end up with a classifier that makes fair predictions when it comes to race as a sensitive attribute. Decent outcome!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bbMmiPvikmQ8"
      },
      "source": [
        "## 5. Summary\n",
        " \n",
        "In this tutorial,  we have shown how to reduce the bias and force our model into making fair predictions by `Adversarial Debiasing` procedure (in-processing technique). And yes, making fair predictions comes at a cost: it will reduce the performance of our model (hopefully, only by a little). However, in many cases, this will be a relatively small price to pay."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3c2Llh8fkmQ9"
      },
      "source": [
        "## References"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HhMCPBhkkmQ9"
      },
      "source": [
        "1. [Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees](https://arxiv.org/abs/1806.06055)\n",
        "2. [Prejudice Remover Regularizer](https://link.springer.com/content/pdf/10.1007%2F978-3-642-33486-3_3.pdf)\n",
        "3. [Mitigating Unwanted Biases with Adversarial Learning](https://arxiv.org/pdf/1801.07593.pdf)\n",
        "4. [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)\n",
        "5. [Adult — UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/Adult)\n",
        "6. [Fairness Constraints: Mechanisms for Fair Classification](https://arxiv.org/pdf/1507.05259.pdf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "adversarial_debiasing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
