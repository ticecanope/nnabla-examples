{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6D2fqYKdCKDh"
   },
   "source": [
    "# Attention Branch Network\n",
    "\n",
    "1. [Introduction](#1.Introduction)\n",
    "2. [Training](#2.Training)\n",
    "3. [Classfication](#3.Classfication)\n",
    "4. [Summary](#4.Summary)\n",
    "5. [References](#5.References)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "The attention map for visual explanation represents a high response value as the attention location in image recognition. This attention region significantly improves the performance of CNN by introducing an attention mechanism that focuses on a specific region in an image. \n",
    "\n",
    "\n",
    "This example demonstrates [Attention Branch Network](https://openaccess.thecvf.com/content_CVPR_2019/html/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019_paper.html), which extends a response-based visual explanation model by introducing a branch structure with an attention mechanism."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SBi_qqAKCKDl"
   },
   "source": [
    "## Preparation\n",
    "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime → change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48613,
     "status": "ok",
     "timestamp": 1647332894197,
     "user": {
      "displayName": "yuta akutsu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09017152410927991052"
     },
     "user_tz": -540
    },
    "id": "MWNJN0kPxQQ8",
    "outputId": "524ef0c8-f1ad-4093-9ded-2859d43a6348"
   },
   "outputs": [],
   "source": [
    "# May show warnings for newly imported packages if run in Colab default python environment.\n",
    "# Please click the `RESTART RUNTIME` to run the following script correctly.\n",
    "# The error message of conflicts is acceptable.\n",
    "!pip install nnabla-ext-cuda116\n",
    "!git clone https://github.com/sony/nnabla-examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nnabla-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsible_ai.attention_branch_network.train import train\n",
    "from responsible_ai.attention_branch_network.infer import infer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_kolxhSl5nnH"
   },
   "source": [
    "# 2. Training\n",
    "Let's start training！\n",
    "\n",
    "The model ABN (Attention Branch Network) is composed based on ResNet110 model. The commented-out code is for comparison as standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5127680,
     "status": "ok",
     "timestamp": 1647338104457,
     "user": {
      "displayName": "yuta akutsu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09017152410927991052"
     },
     "user_tz": -540
    },
    "id": "8gKhrE-FTvP6",
    "outputId": "eec612bc-bda7-4222-eaf3-8f382d152c8e"
   },
   "outputs": [],
   "source": [
    "# train(model=\"resnet110\", train_epochs=5)\n",
    "train(model=\"abn\", train_epochs=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6kowfhOuCKD2"
   },
   "source": [
    "# 3. Classification\n",
    "Let's classify the images!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will show the inference accuracy of test data of CIAFR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80490,
     "status": "ok",
     "timestamp": 1647338440020,
     "user": {
      "displayName": "yuta akutsu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09017152410927991052"
     },
     "user_tz": -540
    },
    "id": "5i_8xjXSP_93",
    "outputId": "142bbaf9-c2c7-4efa-9cd4-4d56d98f2889"
   },
   "outputs": [],
   "source": [
    "# infer(model=\"resnet110\", parameter=\"resnet110_best_params.h5\")\n",
    "infer(model=\"abn\", parameter=\"abn_best_params.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see the results of 300 epoch training although the training above is just 5 epochs for demo.<br>\n",
    "The table below shows a comparison of the inference accuracy of ResNet110 and ABN (trained 300 epochs).The best performance of ResNet110 is 94.27% (280 epoch) while the performance of ABN at the same epoch is 94.41%. We can see ABN accuracy surpasses that of ResNet110 for almost all epoch point.<br>\n",
    "In this example, ABN model is based on ResNet110 with an attention branch incorporated in the model structure of ResNet110. This indicates that the performance of ResNet110 is improved due to attention mechanism brought by the attention map in attention branch in ABN.\n",
    "\n",
    "<br>\n",
    "\n",
    "Epoch | 0 | 5 | 10 | 20 | 30 | 40 | 50 | 60 | 70 | 80 | 90 | 100 | 110 | 120 | 130 | 140 | 150 | 160 | 170 | 180 | 190 | 200 | 210 | 220 | 230 | 240 | 250 | 260 | 270 | 280 | 290 | 300 | \n",
    "--| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "Accuracy (ABN) | 48.94 | 79.33 | 83.11 | 87.00 | 88.43 | 89.15 | 89.30 | 89.67 | 89.71 | 89.77 | 89.78 | 89.66 | 90.33 | 90.88 | 89.98 | 90.25 | 93.45 | 93.95 | 94.12 | 93.81 | 93.94 | 93.88 | 94.03 | 93.85 | 94.19 | 94.27 | 94.39 | 94.33 | 94.18 | 94.41 | 94.34 | 94.31 |\n",
    "Accuracy (ResNet110) | 48.31 | 78.19 | 82.27 | 86.06 | 87.90 | 88.17 | 88.72 | 89.49 | 89.79 | 89.53 | 89.67 | 89.90 | 90.32 | 90.17 | 90.25 | 90.54 | 93.26 | 93.82 | 93.79 | 93.57 | 93.89 | 93.74 | 93.76 | 93.66 | 94.14 | 94.10 | 94.25 | 94.21 | 94.13 | 94.27 | 94.22 | 94.25 |\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"../responsible_ai/attention_branch_network/images/acc.png\"　width=400 height=400><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will visually see the transition of attention map.<br>The table below shows the Attention map for each 10 epochs. You can find that the position of attention is concentrated on the object as learning progresses. <br>\n",
    "\n",
    "\n",
    "epoch | Original Image | 0&ensp;&ensp; | 10&ensp; | 20&ensp;  | 30&ensp; | 40&ensp; | 50&ensp; | 60&ensp; | 70&ensp;| 80&ensp; | 90&ensp; | 100 | 110 | 120 | 130 | 140 | 150 | 160 | 170 | 180 | 190 | 200 | 210 | 220 | 230 | 240 | 250 | 260 | 270 | 280 | 290 | 300 | \n",
    "--| :-: | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "Attention Map | <img src=\"../responsible_ai/attention_branch_network/images/00_original_image.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/01_attention_map_epoch_0.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/02_attention_map_epoch_10.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/03_attention_map_epoch_20.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/04_attention_map_epoch_30.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/05_attention_map_epoch_40.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/06_attention_map_epoch_50.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/07_attention_map_epoch_60.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/08_attention_map_epoch_70.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/09_attention_map_epoch_80.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/10_attention_map_epoch_90.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/11_attention_map_epoch_100.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/12_attention_map_epoch_110.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/13_attention_map_epoch_120.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/14_attention_map_epoch_130.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/15_attention_map_epoch_140.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/16_attention_map_epoch_150.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/17_attention_map_epoch_160.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/18_attention_map_epoch_170.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/19_attention_map_epoch_180.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/20_attention_map_epoch_190.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/21_attention_map_epoch_200.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/22_attention_map_epoch_210.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/23_attention_map_epoch_220.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/24_attention_map_epoch_230.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/25_attention_map_epoch_240.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/26_attention_map_epoch_250.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/27_attention_map_epoch_260.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/28_attention_map_epoch_270.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/29_attention_map_epoch_280.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/30_attention_map_epoch_290.png\" width=25 height=25> | <img src=\"../responsible_ai/attention_branch_network/images/31_attention_map_epoch_300.png\" width=25 height=25> | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summary\n",
    "\n",
    "In this tutorial, we have demonstrated the accuracy of a model can be improved using attention branch, which brings the effect of attention mechanism to the model.<br>\n",
    "In addition, the attention map in attention branch can also be used to visually interpret the result at the same time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. References\n",
    "1. \"Attention Branch Network: Learning of Attention Mechanism for Visual Explanation\". Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "attention_branch_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7785a24edc7e58c365f1249e735748035edd160a29e700a39b3f9c38bbc0ecd8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
