{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sKAbTFrlGF0O"
   },
   "source": [
    "# Eigen-CAM\n",
    "This example interactively demonstrates Eigen-CAM using nnabla's pre-trained model.\n",
    "\n",
    "[Mohammed Bany, Muhammad, Mohammed Yeasin. Eigen-CAM: Class Activation Map using Principal Components. CoRR, 2018.](https://arxiv.org/abs/2008.00299)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij3zQo9cG8le"
   },
   "source": [
    "Where does machine learning look at images to make decisions? This paper describes Eigen-CAM, an algorithm that uses heat maps to indicate the image areas used as the basis for judgment in image classification by CNN [R.R. Selvaraju + 2016]. The principle behind Eigen-CAM is to visualize the information on which the decision is based by displaying the location of the large input gradient to the final convolution layer. Eigen-CAM allows you to visualize areas of relevance for each prediction class. In the final output, the feature maps are decomposed into EigenVectors using SVD. The feature is that the operation is very light with no back propagation, so it can be operated at high speed. \n",
    "\n",
    " \"![Eigen-CAM algorithm](https://github.com/sony/nnabla-examples/raw/master/responsible_ai/eigencam/images/overview.png)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vNZqVzzZGF0X"
   },
   "source": [
    "# Preparation\n",
    "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime → change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSxlbQDfGF0Y"
   },
   "outputs": [],
   "source": [
    "# May show warnings for newly imported packages if run in Colab default python environment.\n",
    "# Please click the `RESTART RUNTIME` to run the following script correctly.\n",
    "# The error message of conflicts is acceptable.\n",
    "!pip install nnabla-ext-cuda116\n",
    "!git clone https://github.com/sony/nnabla-examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nnabla-examples/responsible_ai/eigencam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NKII4V6-GF0a"
   },
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R16x__BQGF0b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnabla as nn\n",
    "import nnabla.functions as F\n",
    "from nnabla.utils.image_utils import imread, imresize\n",
    "from nnabla.models.imagenet import VGG16\n",
    "\n",
    "from nnabla.ext_utils import get_extension_context\n",
    "from utils import overlay_images, resize_image_for_yolo, decode_img_str, encode_img\n",
    "\n",
    "ctx = get_extension_context('cudnn')\n",
    "nn.set_default_context(ctx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EUKDyTPIGF0c"
   },
   "source": [
    "## Image Preparation \n",
    "Download image to apply Grad-CAM for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EYYpg1dGF0c"
   },
   "outputs": [],
   "source": [
    "url = 'https://upload.wikimedia.org/wikipedia/commons/4/4e/A_crab_spider_on_a_flower_preying_upon_a_euglossine_bee%2C_while_a_butterfly_looks_for_nectar.jpg'\n",
    "img_path = 'input_flower_moth_spider.jpg'\n",
    "if not os.path.isfile(img_path):\n",
    "    tgt = urllib.request.urlopen(url).read()\n",
    "    with open(img_path, mode='wb') as f:\n",
    "        f.write(tgt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5HLuqJf-GF0d"
   },
   "source": [
    "Take a look at what the image looks like.  \n",
    "We can see a flower in the middle on which a butterfly rests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ralmYCVTGF0d"
   },
   "outputs": [],
   "source": [
    "img = imread(img_path, size=(224, 224), channel_first=True)\n",
    "plt.imshow(img.transpose(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ulz09HAXGF0e"
   },
   "source": [
    "## Network Definition\n",
    "Loading the model is very simple.<br>\n",
    "You can choose other models such as `VGG11`, `VGG13`, by specifying the model's name as an argument. Of course, you can choose other pretrained models as well. See the [Docs](https://nnabla.readthedocs.io/en/latest/python/api/models/imagenet.html).\n",
    "\n",
    "**NOTE**: If you use the `VGG16` for the first time, nnabla will automatically download the weights from `https://nnabla.org` and it may take up to a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oty7lljEGF0f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r240UI5lGF0g"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "x = nn.Variable((batch_size,) + model.input_shape)\n",
    "vgg = model(x, returns_net=True)\n",
    "vgg_variables = vgg.variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BuJ2mo1hGF0g"
   },
   "source": [
    "We now define the input, and extract the necessary outputs.  \n",
    "middle_layer: the last convolution layer  \n",
    "pred: final output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gF_Cv-8pGF0h"
   },
   "outputs": [],
   "source": [
    "input_name = list(vgg.inputs.keys())[0]\n",
    "vgg_variables[input_name].d = img\n",
    "middle_layer = vgg_variables['VGG16/ReLU_13']\n",
    "pred = vgg_variables[\"VGG16/Affine_3\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy_Pgj_sGF0h"
   },
   "source": [
    "Let's see how the model predicted the image.  \n",
    "We can see the model classified the image as we expect.  \n",
    "Labels regarding butterfly comes high, while flower is also recognized although it is14th ranked probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqLuq4srGF0i"
   },
   "outputs": [],
   "source": [
    "pred.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs9yJ2uAGF0i"
   },
   "outputs": [],
   "source": [
    "predicted_labels = np.argsort(-pred.d[0])\n",
    "for i, predicted_label in enumerate(predicted_labels[:15]):\n",
    "        print(f'Top {i+1}, Label index: {predicted_label},  Label name: {model.category_names[predicted_label]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3QsAoeSLGF0i"
   },
   "source": [
    "## Eigen-CAM Computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YYsvM6e7GF0k"
   },
   "source": [
    "Let's compute the heatmap using the gradient with respect to the last convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FS5bgH7VxQ6p"
   },
   "outputs": [],
   "source": [
    "def eigencam(middle_layer, eigenvector_index=0):\n",
    "    \"\"\"\n",
    "    Calculate EigenCAM.\n",
    "    Parameters\n",
    "    ----------\n",
    "    middle_layer: nn.Variable\n",
    "        The layer of interest to apply EigenCAM\n",
    "    Returns\n",
    "    ----------\n",
    "    heatmap: ndarray\n",
    "        2D array of same size as width and height of middle_layer\n",
    "    \"\"\"\n",
    "    conv_layer_output = middle_layer.d\n",
    "    heatmap = get_2d_projection(conv_layer_output, eigenvector_index)\n",
    "    max_v, min_v = np.max(heatmap), np.min(heatmap)\n",
    "    if max_v != min_v:\n",
    "        heatmap = (heatmap - min_v) / (max_v - min_v)\n",
    "    return heatmap[0]\n",
    "\n",
    "def get_2d_projection(activation_batch, eigenvector_index=0):\n",
    "    # https://github.com/jacobgil/pytorch-grad-cam/blob/master/pytorch_grad_cam/utils/svd_on_activations.py\n",
    "    # TBD: use pytorch batch svd implementation\n",
    "    activation_batch[np.isnan(activation_batch)] = 0\n",
    "    activation_batch[np.isinf(activation_batch)] = 0\n",
    "    \n",
    "    projections = []\n",
    "    for activations in activation_batch:\n",
    "        reshaped_activations = (activations).reshape(\n",
    "            activations.shape[0], -1).transpose()\n",
    "        # Centering before the SVD seems to be important here,\n",
    "        # Otherwise the image returned is negative\n",
    "        reshaped_activations = reshaped_activations - \\\n",
    "            reshaped_activations.mean(axis=0)\n",
    "        U, S, VT = np.linalg.svd(reshaped_activations, full_matrices=True)\n",
    "        projection = reshaped_activations @ VT[eigenvector_index, :]\n",
    "        projection = projection.reshape(activations.shape[1:])\n",
    "        projections.append(projection)\n",
    "    return np.float32(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VimBZEQAGF0k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatmap = eigencam(middle_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DxMt28WUGF0l"
   },
   "source": [
    "## Visualization\n",
    "Take a look at how the heatmap looks like in the layer of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47FUNitSGF0l"
   },
   "outputs": [],
   "source": [
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJvrf5naGF0m"
   },
   "source": [
    "Then we overlay the heatmap onto the original image to understand where the model focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTanVEd5GF0m"
   },
   "outputs": [],
   "source": [
    "base_img = imread(img_path, size=(224, 224))\n",
    "overlaid_img_butterfly = overlay_images(base_img, heatmap)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq0vT4KCGF0n"
   },
   "source": [
    "Now we overlay the heatmap onto the original image to understand where the model focused.  \n",
    "We can speculate the model recognized the butterfly, focusing on its wing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2YIazBrGF0n"
   },
   "outputs": [],
   "source": [
    "plt.imshow(overlaid_img_butterfly)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RyAZv__QGF0n"
   },
   "source": [
    "Let's visualize another eigenvector visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stzLVKIfGF0o"
   },
   "outputs": [],
   "source": [
    "heatmap = eigencam(middle_layer, eigenvector_index=1)\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YcmoZQpjGF0p"
   },
   "source": [
    "We can see the model focus is widely spread comparing to than for butterfly as if the heatmap wrapped the flower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B17EwaNSGF0p"
   },
   "outputs": [],
   "source": [
    "overlaid_img = overlay_images(base_img, heatmap)\n",
    "plt.imshow(overlaid_img)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QE6XI60cUnfj"
   },
   "source": [
    "Finally, let's visualize top-3 eigen vectors. We can see each heatmap focuses on a butterfly, flower and spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO7LwIlw3ZoF"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 40))\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 1)\n",
    "ax.imshow(base_img)\n",
    "ax.axis(\"off\")\n",
    "plt.title(\"original image\")\n",
    "\n",
    "for i in range(3):\n",
    "    ax = fig.add_subplot(1, 4, i+2)\n",
    "    heatmap = eigencam(middle_layer, eigenvector_index=i)\n",
    "    overlaid_img = overlay_images(base_img, heatmap)\n",
    "    plt.title(f\"using\\n  {i+1}st singular vector\")\n",
    "    ax.imshow(overlaid_img)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xr6_k01WL0bR"
   },
   "source": [
    "# Detection using yoloV2\n",
    "### Preparation\n",
    "Let's also download pre-trained weight parameters and auxiliary file. We will also convert the weight parameters to .h5 format to make it compatible with NNabla. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxuK5eDgaQgM"
   },
   "outputs": [],
   "source": [
    "%cd ../../object-detection/yolov2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtXajM6FL6Ml"
   },
   "outputs": [],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov2.weights\n",
    "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U97jgkava7TS"
   },
   "outputs": [],
   "source": [
    "!python convert_yolov2_weights_to_nnabla.py --input yolov2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZEsvQMfbQYJ"
   },
   "outputs": [],
   "source": [
    "import yolov2\n",
    "from draw_utils import DrawBoundingBoxes\n",
    "from arg_utils import get_anchors_by_name_or_parse\n",
    "from yolov2_detection import draw_bounding_boxes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WbYjKb2rwzSi"
   },
   "source": [
    "Set each parameters for Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JMBUksQeR9c"
   },
   "outputs": [],
   "source": [
    "class_names = \"coco.names\"\n",
    "classes = 80\n",
    "weights = 'yolov2.h5'\n",
    "width = 608\n",
    "height = 608 \n",
    "anchors = np.array(get_anchors_by_name_or_parse(\"coco\"))\n",
    "num_anchors = len(anchors) // 2\n",
    "anchors = np.array(anchors).reshape(-1, 2)\n",
    "thresh = 0.5\n",
    "nms = 45\n",
    "nms_per_class = True\n",
    "\n",
    "names = np.genfromtxt(class_names, dtype=str, delimiter='?')\n",
    "rng = np.random.RandomState(1223)\n",
    "colors = rng.randint(0, 256, (classes, 3)).astype(np.uint8)\n",
    "colors = [tuple(c.tolist()) for c in colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZHsqPBwekEB"
   },
   "outputs": [],
   "source": [
    "  # Load parameter\n",
    "  nn.clear_parameters()\n",
    "  _ = nn.load_parameters(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SUTNAG9UGxKy"
   },
   "source": [
    "#### Build a YOLO v2 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaLlyYyHe8FS"
   },
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "x = nn.Variable((1, 3, width, width))\n",
    "y = yolov2.yolov2(x, num_anchors, classes,\n",
    "                  test=True, feature_dict=feature_dict)\n",
    "y = yolov2.yolov2_activate(y, num_anchors, anchors)\n",
    "y = F.nms_detection2d(y, thresh, nms, nms_per_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f29GBZHe8sOC"
   },
   "source": [
    "# Upload Image\n",
    "Run the following cell to upload your own image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgfcOs2w8Az-"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "upload_img = files.upload()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6C308r7Pnff1"
   },
   "source": [
    "Let's rename the image for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UHI_yu48GtW"
   },
   "outputs": [],
   "source": [
    "ext = os.path.splitext(list(upload_img.keys())[-1])[-1]\n",
    "os.rename(list(upload_img.keys())[-1], \"input_image{}\".format(ext)) \n",
    "input_img = \"input_image\" + ext\n",
    "img_orig = imread(input_img, num_channels=3)\n",
    "im_h, im_w, _ = img_orig.shape\n",
    "img, new_w, new_h = resize_image_for_yolo(img_orig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yvvsnx-YskD_"
   },
   "source": [
    "# Execute YOLO v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSrRQ8sp6oA4"
   },
   "outputs": [],
   "source": [
    "in_img = img.transpose(2, 0, 1).reshape(1, 3, width, width)\n",
    "x.d = in_img\n",
    "y.forward(clear_buffer=True)\n",
    "bboxes = y.d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbzeJ9HuvOrt"
   },
   "outputs": [],
   "source": [
    "img_draw = draw_bounding_boxes(img_orig, bboxes, im_w, im_h, names, colors, new_w * 1.0 / width, new_h * 1.0 / height, 0.5)\n",
    "plt.imshow(img_draw)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IqH8y-lLakLz"
   },
   "source": [
    "# Get middle layer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ud7OgV60VuVg"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class get_middle_variables:\n",
    "    def __init__(self):\n",
    "        self.middle_vars_dict = OrderedDict()\n",
    "        self.middle_layer_count_dict = OrderedDict()\n",
    "    def __call__(self, f):\n",
    "        if f.name in self.middle_layer_count_dict:\n",
    "            self.middle_layer_count_dict[f.name] += 1\n",
    "        else:\n",
    "            self.middle_layer_count_dict[f.name] = 1\n",
    "        key = f.name + '_{}'.format(self.middle_layer_count_dict[f.name])\n",
    "        self.middle_vars_dict[key] = f.outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92Wlps21Vyo8"
   },
   "outputs": [],
   "source": [
    "GET_MIDDLE_VARIABLES_CLASS = get_middle_variables()\n",
    "y.visit(GET_MIDDLE_VARIABLES_CLASS)\n",
    "middle_vars = GET_MIDDLE_VARIABLES_CLASS.middle_vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlsEzvg8a-m0"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 1)\n",
    "ax.imshow(img_orig)\n",
    "ax.axis(\"off\")\n",
    "plt.title(\"original image\")\n",
    "\n",
    "for i in range(3):\n",
    "    ax = fig.add_subplot(1, 4, i+2)\n",
    "    heatmap = eigencam(middle_vars['ConvolutionCudaCudnn_23'], eigenvector_index=i) #set variables key\n",
    "    overlaid_img = overlay_images(img_orig, heatmap)\n",
    "    plt.title(f\"using\\n  {i+1}st singular vector\")\n",
    "    ax.imshow(overlaid_img)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IK9YNVIIxOs5"
   },
   "source": [
    "# Real time Visualization for YoloV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlOSinyUmTaY"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "!pip install bottle\n",
    "!pip install bottle_websocket\n",
    "!pip install gevent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ifq0b0q7dlE1"
   },
   "source": [
    "### Set processing callback for notebook and render webcamera using javescript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qILaGzyx88Yh"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import base64\n",
    "from google.colab import output\n",
    "from io import BytesIO\n",
    "\n",
    "def run(img_str):\n",
    "    #decode to image\n",
    "    decimg = decode_img_str(img_str)\n",
    "    im_h, im_w, _ = decimg.shape\n",
    "\n",
    "    ##### detection and visualization \n",
    "    img, new_w, new_h = resize_image_for_yolo(decimg)\n",
    "    in_img = img.transpose(2, 0, 1).reshape(1, 3, width, width)\n",
    "    x.d = in_img\n",
    "    y.forward(clear_buffer=True)\n",
    "    bboxes = y.d[0]\n",
    "\n",
    "    GET_MIDDLE_VARIABLES_CLASS = get_middle_variables()\n",
    "    y.visit(GET_MIDDLE_VARIABLES_CLASS)\n",
    "    middle_vars = GET_MIDDLE_VARIABLES_CLASS.middle_vars_dict\n",
    "\n",
    "    heatmap = eigencam(middle_vars[\"ConvolutionCudaCudnn_23\"], eigenvector_index=0)\n",
    "  \n",
    "    img_draw = draw_bounding_boxes(\n",
    "        decimg, bboxes, im_w, im_h, names, colors, new_w * 1.0 / width, new_h * 1.0 / height, 0.4)\n",
    "    \n",
    "    out_img = overlay_images(img_draw, heatmap)\n",
    "\n",
    "    #encode to string\n",
    "    img_str = encode_img(out_img)\n",
    "\n",
    "    return IPython.display.JSON({'img_str': img_str})\n",
    "\n",
    "output.register_callback('notebook.run', run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxjlH7bbMSny"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "\n",
    "def use_webcam(quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function useCam(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      document.body.appendChild(div);\n",
    "\n",
    "      // camera btn\n",
    "      var current_deviceId = \"test\";\n",
    "      var new_deviceId = \"test\";\n",
    "      const camera_div = document.createElement('div');\n",
    "      document.body.appendChild(camera_div);\n",
    "      //get deviceIds\n",
    "      navigator.mediaDevices.enumerateDevices()\n",
    "      .then(function(devices) {\n",
    "          devices.forEach(function(device, i) {\n",
    "              //exit button\n",
    "              if (device.deviceId != \"\") {\n",
    "                const canera_btn = document.createElement('button');\n",
    "                canera_btn.textContent = \"camera\" + i;\n",
    "                canera_btn.onclick = function() {\n",
    "                  new_deviceId = device.deviceId\n",
    "                };\n",
    "                camera_div.appendChild(canera_btn);\n",
    "              }\n",
    "          });\n",
    "      })\n",
    "      .catch(function(err) {\n",
    "        console.log(err.name + \": \" + err.message);\n",
    "      });\n",
    "\n",
    "      //video element\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'None';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: { deviceId: current_deviceId } });\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      //canvas for display. frame rate is depending on display size and jpeg quality.\n",
    "      display_size = 500\n",
    "      const src_canvas = document.createElement('canvas');\n",
    "      src_canvas.width  = display_size;\n",
    "      src_canvas.height = display_size * video.videoHeight / video.videoWidth;\n",
    "      const src_canvasCtx = src_canvas.getContext('2d');\n",
    "      src_canvasCtx.translate(src_canvas.width, 0);\n",
    "      src_canvasCtx.scale(-1, 1);\n",
    "      div.appendChild(src_canvas);\n",
    "\n",
    "      const dst_canvas = document.createElement('canvas');\n",
    "      dst_canvas.width  = src_canvas.width;\n",
    "      dst_canvas.height = src_canvas.height;\n",
    "      const dst_canvasCtx = dst_canvas.getContext('2d');\n",
    "      div.appendChild(dst_canvas);\n",
    "\n",
    "      //exit button\n",
    "      const btn_div = document.createElement('div');\n",
    "      document.body.appendChild(btn_div);\n",
    "      const exit_btn = document.createElement('button');\n",
    "      exit_btn.textContent = 'Exit';\n",
    "      var exit_flg = true\n",
    "      exit_btn.onclick = function() {exit_flg = false};\n",
    "      btn_div.appendChild(exit_btn);\n",
    "\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      var send_num = 0\n",
    "      // loop\n",
    "      _canvasUpdate();\n",
    "      async function _canvasUpdate() {\n",
    "\n",
    "            src_canvasCtx.drawImage(video, 0, 0, video.videoWidth, video.videoHeight, 0, 0, src_canvas.width, src_canvas.height);     \n",
    "            if (send_num<1){\n",
    "                send_num += 1\n",
    "                const img = src_canvas.toDataURL('image/jpeg', quality);\n",
    "                const result = google.colab.kernel.invokeFunction('notebook.run', [img], {});\n",
    "                result.then(function(value) {\n",
    "                    parse = JSON.parse(JSON.stringify(value))[\"data\"]\n",
    "                    parse = JSON.parse(JSON.stringify(parse))[\"application/json\"]\n",
    "                    parse = JSON.parse(JSON.stringify(parse))[\"img_str\"]\n",
    "                    var image = new Image()\n",
    "                    image.src = parse;\n",
    "                    image.onload = function(){dst_canvasCtx.drawImage(image, 0, 0)}\n",
    "                    send_num -= 1\n",
    "                })\n",
    "            }\n",
    "            if (exit_flg){\n",
    "                requestAnimationFrame(_canvasUpdate);   \n",
    "            }else{\n",
    "                stream.getVideoTracks()[0].stop();\n",
    "            }\n",
    "            if (new_deviceId != current_deviceId) {\n",
    "              console.log(\"change camera!\");\n",
    "              current_deviceId = new_deviceId;\n",
    "              const stream = await navigator.mediaDevices.getUserMedia({video: { deviceId: current_deviceId } });\n",
    "              video.srcObject = stream;\n",
    "              await video.play();\n",
    "            }\n",
    "      };\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('useCam({})'.format(quality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3J0GwRp09EqX"
   },
   "outputs": [],
   "source": [
    "use_webcam()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Eigen-CAM.ipynb のコピー",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/sony/nnabla-examples/blob/master/interactive-demos/gradcam.ipynb",
     "timestamp": 1646199145054
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
