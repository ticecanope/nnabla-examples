{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot classification example in nnabla CLIP"
      ],
      "metadata": {
        "id": "Fc5UziTe-15v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OFzm8AQXZTeG"
      },
      "outputs": [],
      "source": [
        "#@title Set up environment in Colab (you can skip this if you have done it.)\n",
        "!pip install nnabla-ext-cuda114\n",
        "!pip install -U --no-deps numpy scipy\n",
        "!git clone https://github.com/sony/nnabla-examples/\n",
        "CLIP_ROOT = 'nnabla-examples/vision-and-language/clip'\n",
        "!cd {CLIP_ROOT} && pip install -r requirements.txt\n",
        "!mkdir -p data\n",
        "!cd {CLIP_ROOT} && if [ ! -f data/ViT-L-14.h5 ] ; then curl -o data/ViT-L-14.h5 https://nnabla.org/pretrained-models/nnabla-examples/vision-and-language/clip/ViT-L-14.h5; fi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CLIP pretrained model"
      ],
      "metadata": {
        "id": "pAPyBG_s_KzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd nnabla-examples/vision-and-language/clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep_IaYFr2Zch",
        "outputId": "acfc1cf6-5561-4b82-856a-dd0caaca8b41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nnabla-examples/vision-and-language/clip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nnabla as nn\n",
        "from  nnabla.ext_utils import get_extension_context\n",
        "nn.set_default_context(get_extension_context('cudnn'))"
      ],
      "metadata": {
        "id": "4V_ZyNOI5oOM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from clip import CLIP\n",
        "from demo_zero_shot_classification import download_image\n",
        "clip = CLIP(model_path='data/ViT-L-14.h5')"
      ],
      "metadata": {
        "id": "Jmj3cIfr5XVs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run zero-shot classification\n"
      ],
      "metadata": {
        "id": "KMqpqW6I_Ylf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Provide an image url as input.\n",
        "import IPython\n",
        "image_url = 'https://images.pexels.com/photos/5682847/pexels-photo-5682847.jpeg?auto=compress&cs=tinysrgb&w=600' #@param {type:\"string\"}\n",
        "IPython.display.Image(url=image_url, height=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "cellView": "form",
        "id": "8tcoIG_c1ZCH",
        "outputId": "28e80bdf-a848-4e25-a049-63e842108222"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://images.pexels.com/photos/5682847/pexels-photo-5682847.jpeg?auto=compress&cs=tinysrgb&w=600\" height=\"256\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define zero-shot categories.\n",
        "categories = [f'A photo of a person with {color} hair' for color in ['black', 'white', 'brown', 'brond', 'red', 'blue', 'green', 'pink', 'orange', 'yellow']]\n",
        "for c in categories:\n",
        "  print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEo6iRCW4bwo",
        "outputId": "42bff68b-e35c-4617-eaa1-f7ab90157a3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A photo of a person with black hair\n",
            "A photo of a person with white hair\n",
            "A photo of a person with brown hair\n",
            "A photo of a person with brond hair\n",
            "A photo of a person with red hair\n",
            "A photo of a person with blue hair\n",
            "A photo of a person with green hair\n",
            "A photo of a person with pink hair\n",
            "A photo of a person with orange hair\n",
            "A photo of a person with yellow hair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run zero-shot classification for the image and the specified categories above.\n",
        "image_bytes = download_image(image_url, return_bytes=True)\n",
        "with nn.auto_forward(), nn.no_grad():\n",
        "  probs = clip.run(image_bytes, categories)\n",
        "\n",
        "for cate, prob in sorted(zip(categories, probs), key=lambda x: x[1], reverse=True):\n",
        "  print(f'{cate}: {prob * 100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_XW1GI_eKH7",
        "outputId": "22837874-3efa-4ca0-f256-77ea0aaf0e56"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A photo of a person with white hair: 98.05\n",
            "A photo of a person with yellow hair: 1.10\n",
            "A photo of a person with blue hair: 0.43\n",
            "A photo of a person with brond hair: 0.30\n",
            "A photo of a person with green hair: 0.09\n",
            "A photo of a person with pink hair: 0.03\n",
            "A photo of a person with black hair: 0.00\n",
            "A photo of a person with brown hair: 0.00\n",
            "A photo of a person with orange hair: 0.00\n",
            "A photo of a person with red hair: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-step execution"
      ],
      "metadata": {
        "id": "tR0pkarbC5L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess image and text\n",
        "image_bytes.seek(0)\n",
        "with nn.auto_forward(), nn.no_grad():\n",
        "  image = clip.preprocess_image(image_bytes)\n",
        "  tokens = clip.preprocess_texts(categories)\n",
        "print(f'Image shape: {image.shape}, tokens shape: {tokens.shape}')\n",
        "print(f'First category \"{categories[0]}\" is tokenized as:\\n{tokens.d[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyYD0eZJ6b_r",
        "outputId": "6d6939bb-207f-4c5f-f66f-ecc7efaa0b1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (1, 3, 224, 224), tokens shape: (10, 77)\n",
            "First category \"A photo of a person with black hair\" is tokenized as:\n",
            "[49406   320  1125   539   320  2533   593  1449  2225 49407     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode image and text by pre-trained CLIP encoders\n",
        "with nn.auto_forward(), nn.no_grad():\n",
        "  image_features = clip.encode_image(image)\n",
        "  text_features = clip.encode_text(tokens)\n",
        "print(f'image_features shape: {image_features.shape}, text_features shape: {text_features.shape}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8yssXAZCwnw",
        "outputId": "652cd605-8c14-42a8-e216-eee7083f6ce3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_features shape: (1, 768), text_features shape: (10, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute similarity (normalized by softmax) between the image and each of categories\n",
        "with nn.auto_forward(), nn.no_grad():\n",
        "  probs = clip.probabilities(image_features, text_features)\n",
        "print(f' probs shape: {probs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVBwzPb9CzW8",
        "outputId": "4129339b-7514-4e2b-ce9f-e22ac328b8ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " probs shape: (1, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for cate, prob in sorted(zip(categories, probs.d[0]), key=lambda x: x[1], reverse=True):\n",
        "  print(f'{cate}: {prob * 100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAh3Z9QEC1X3",
        "outputId": "14fb6bf9-f310-4efc-88eb-a11abcd4498a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A photo of a person with white hair: 98.05\n",
            "A photo of a person with yellow hair: 1.10\n",
            "A photo of a person with blue hair: 0.43\n",
            "A photo of a person with brond hair: 0.30\n",
            "A photo of a person with green hair: 0.09\n",
            "A photo of a person with pink hair: 0.03\n",
            "A photo of a person with black hair: 0.00\n",
            "A photo of a person with brown hair: 0.00\n",
            "A photo of a person with orange hair: 0.00\n",
            "A photo of a person with red hair: 0.00\n"
          ]
        }
      ]
    }
  ]
}